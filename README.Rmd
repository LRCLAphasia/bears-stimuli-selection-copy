---
output: github_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# bears-stimuli-selection

last updated on `r Sys.Date()`.

This project uses {renv}. To install relevant packages, run:

```{r, eval = FALSE}
install.packages("renv")
renv::restore()
```

*note that the development version of the {anticlust} package is required, and there
is a current issue with the {digest} package on apple silicon m1/m2 (a dependency of
rmarkdown). If installation fails, run `install.packages('digest', repos = c('https://eddelbuettel.r-universe.dev', 'https://cloud.r-project.org'))`*



## organization

```{r}
fs::dir_tree(recurse = FALSE)
```

- R: *scripts*
- VTT: *source data files*
- bears-stimuli-selection.Rproj: *R project file. must be opened to use scripts*
- check-nouns: *output from VTT files; was hand checked by RAs*
- check-nouns hand corrected: *corrected check-nouns files*
- data: *external or generated data files relevant to the processing stream*
- output: *old generated files not relevant to current processing stream. saved for posterity*

## current state

Files have been cleaned from the VTT transcript files generated by zoom and the final
script (04) will generated balanced lists across 3 conditions and 2 item-types (tx, control)
relatively well. They are balanced for name agreement, item difficulty, and salience in discourse stimuli. They are also balanced for how many were produced in the discourse tasks. 

Here's a concetual outline of the current stimuli selection process: 

1. Given a participants naming ability...
2. generated a predicted probability of a correct response for all items
3. Pick the best 84 items that appear in the naming battery AND discourse stimuli
that are closest to p(correct) = 33%. 
4. Pick the best remaining 96 items regardless of whether they appear in naming and
discoures or just naming. (side note, 84/96 currently used because they result in equal size
conditions/item type groups. we could also do 72/108).
5. Use the anticlustering algorithm to split the items into 3 condition groups of 60, balancing
item difficulty, agreement, discourse salience, and the % in discourse. 
6. Then for each condition (n = 60), split again into sets of 40 (treated) and 20 (contorl)
balancing these same factors. 

## current challenge

The current stimuli selection process is completely naive to the source of the stimuli that appear in discourse. So its possible that we might need to administer all discourse stimuli at each timepoint/condition to assess whether an item is produced at the discourse level. 

We need to incorporate the source of the items that appear in discourse into the stimuli
selection process so that each condition has only a subset of the discourse stimuli (and
I think that these discourse stimuli should only be used once). This is further complicated
by the fact that some items derived from the discourse stimuli appear in multiple discourse stimuli. 

So the initial item selection of 180 words should probably try to try to avoid selecting items
from a discourse stimuli if only a handful are selected from that stimuli. 

The split into 3 groups of 60 items then needs to also divide words by combining specific
discourse stimuli to achieve these 3 groups of 60 words

Then we need to run the same checks on the final stimuli lists comparing treated items vs. 
control items and the three diffierent categories. 

## Update 8/27

- implemented a version doing something along these lines, with a few compromises
- added beginning of a study 2 choices with many more items
- code is documented and cleanish
- still have a number of to-do's



