---
title: "Script to Parse VTT files for BEARS Stimuli Norming"
author: "Rob Cavanaugh Parmitha Chanduri"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: paper
    highlight: kate
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load packages

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(tidytext)
library(textstem)
library(textclean)
# note for koRpus - you will also have to install the
# tree tagger software which is a giant pain...
# https://cran.r-project.org/web/packages/koRpus/vignettes/koRpus_vignette.html
library(udpipe)
library(here)
here()
```

## Read in files

```{r}
dat = read.delim(
  here("VTT", "Participant1_Session1_Corrected.vtt"),
  sep = "\t")
```

## Data cleaning

1. Clean up the VTT files

```{r}
# Select all of the text only - ignore the timestamps and identifiers
txt = dat %>%
  # create two new columns
  # detect the arrow, or the row before the arrow row
  mutate(arrow = ifelse(str_detect(pattern = "-->", string = WEBVTT), 1, 0),
         before_arrow = lead(arrow)) %>%
  # only keep rows with arrow and before_arrow == 0
  filter(arrow == 0, before_arrow == 0) %>%
  # get rid of quotation marks?
  mutate(text = str_remove_all(string = WEBVTT, pattern = '"'))

text_string = txt %>%
  pull(text)

# collapse it all together. 
text = paste(text_string, collapse = " ")
```

2. Get rid of text between curly brackets

```{r}
# replace all the curly brackets and stuff in between them
no_curly = str_replace_all(text, "\\s*\\{[^\\}]+\\}", "")
```

Check the results

```{r}
nocurly_sentences = tibble(
  text = no_curly
) %>%
  unnest_tokens(input = "text", output = "words",
                to_lower = TRUE, token = "regex", pattern = " ")
```

3. Find the participant responses between start and end

```{r}
start_end = nocurly_sentences %>%
  mutate(start = ifelse(str_detect(words, "#start"), 1, 0),
         end = ifelse(str_detect(words, "#end"), 1, 0))
```

4. Get the names of the stimuli out of the sentence tags

```{r}
stimuli_names = start_end %>%
  # what is the tag when start is 1 and then when end is 1
  mutate(tag = case_when(
    start == 1 ~ str_extract(string = words,
                             pattern = "(?<=^|\\s)#[^\\s]+"),
    end == 1   ~ str_extract(string = words,
                             pattern = "(?<=^|\\s)#[^\\s]+"))
  ) %>%
  # separate the tag into #start/#end and the stimuli name
  separate(tag, into = c("start_end", "stimuli"), sep = "_", extra = "merge") %>%
  # remove the # from before #start or #end
  mutate(start_end = str_remove(start_end, "#"))
```

5. Use that info to fill in the names of the stimuli

```{r}
fill_stimuli = stimuli_names %>%
  # make two columns of stimuli that are the same
  mutate(prev_stim = stimuli,
         next_stim = stimuli) %>%
  # fill one of the down and the other one up
  fill(prev_stim, .direction = "down") %>%
  fill(next_stim, .direction = "up") %>%
  # when these two columns are the same, those are the rows of our stimuli
  mutate(stimuli = ifelse(prev_stim == next_stim, prev_stim, stimuli)) %>%
  # we only need these columns
  select(words, stimuli) %>%
  # get rid of rows that are not stimuli
  drop_na(stimuli) %>%
  # remove start and end rows
  filter(
    str_detect(words, "#start", negate = TRUE),
    str_detect(words, "#end", negate = TRUE)
  ) %>%
  # remove everything after the end marker in the row
  # then remove words starting with #start
  # these appear to always be at the beginning
  mutate(# replace contractions
         words = textclean::replace_contraction(words),
         words = textclean::replace_number(words),
         words = str_replace_all(words, "'", " "))
```

5. Put the transcripts together

```{r}
# start with the fill_stimuli data
transcript_by_stim = fill_stimuli %>%
  # for each unique value of stimuli
  group_by(stimuli) %>%
  # paste all the tokens together with a space between them.
  summarize(response = paste(words, collapse = " "))
```

6. Part of speech tagging

This uses the R package "udpipe". It will have to download a part of speech parsing model
the first time you use it on a computer. After that, it should just run. 

```{r}
l = list()

for(i in 1:nrow(transcript_by_stim)){
  l[[i]] = udpipe(x = transcript_by_stim$response[[i]], object = "english")
    
  l[[i]]$stimuli = transcript_by_stim$stimuli[[i]]
}

tagged = bind_rows(l) %>%
  select(stimuli, token, lemma, upos, xpos)

```

7. Get unique nouns for each stimuli

```{r}
unique_nouns = tagged %>%
  filter(upos == "NOUN") %>%
  group_by(stimuli) %>%
  summarize(lemma = unique(lemma))
```

## Finish text cleaning

- [x]  pull out the stimuli name into a column for every row that has a #start or #end
- [x]  fill out so that every sentence within the stimulus is assigned to that task name
- [x]  get rid of any rows that aren't part of a stimulus response by a participant
- [x]  put all the rows for each task together (collapsing)

## Finding the nouns

- [x]  use a 'part of speech tagger' to find all the nouns
- [x]  "lemmatize" the nouns. Lemma is like the "base" form of a word
- [ ]  create a list of nouns used for each person

## Do this for all the files

- tweak the script to read lots of files at once
- do all of the steps to each file
- save the results together

## Steps for continuing

- save this rproject and put it on sharepoint 
- let Parmitha know that I've done this
- Leave some hints on what to try next
- set up a time to meet - next Friday? Parmitha to send times 

For later

```{r}
#separate into words

df = tibble(text = text) %>%
  unnest_tokens(input = "text", output = "tokens",  to_lower = TRUE)
```



- write a script that batch runs all the files
- write a script that produces core lex tables


